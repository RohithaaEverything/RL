{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSMDF9a4pIfE",
        "outputId": "30807195-3b3e-457f-a820-711e7612e5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value table is:\n",
            "[3.778      4.24315789 3.93414387 3.68756757 3.98471616]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple environment with deterministic transitions\n",
        "# For simplicity, let's assume there are 5 states moving from one state to the next gives a reward of 1, with state 4 being terminal\n",
        "class SimpleEnvironment:\n",
        "\tdef __init__(self, num_states=5):\n",
        "\t\tself.num_states = num_states\n",
        "\n",
        "\tdef step(self, state):\n",
        "\t\treward = 0\n",
        "\t\tterminal = False\n",
        "\n",
        "\t\tif state < self.num_states - 1:\n",
        "\t\t\tnext_state = state + 1\n",
        "\t\t\treward = 1\n",
        "\t\telse:\n",
        "\t\t\tnext_state = state\n",
        "\t\t\tterminal = True\n",
        "\n",
        "\t\treturn next_state, reward, terminal\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\treturn 0 # Start from state 0\n",
        "\n",
        "# Define a random policy for the sake of demonstration\n",
        "def random_policy(state, num_actions=5):\n",
        "\treturn np.random.choice(num_actions)\n",
        "\n",
        "# Monte Carlo Policy Evaluation function\n",
        "def monte_carlo_policy_evaluation(policy, env, num_episodes, gamma=1.0):\n",
        "\tvalue_table = np.zeros(env.num_states)\n",
        "\treturns = {state: [] for state in range(env.num_states)}\n",
        "\n",
        "\tfor _ in range(num_episodes):\n",
        "\t\tstate = env.reset()\n",
        "\t\tepisode = []\n",
        "\t\t# Generate an episode\n",
        "\t\twhile True:\n",
        "\t\t\taction = policy(state)\n",
        "\t\t\tnext_state, reward, terminal = env.step(action)\n",
        "\t\t\tepisode.append((state, reward))\n",
        "\t\t\tif terminal:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tstate = next_state\n",
        "\n",
        "\t\t# Calculate the return and update the value table\n",
        "\t\tG = 0\n",
        "\t\tfor state, reward in reversed(episode):\n",
        "\t\t\tG = gamma * G + reward\n",
        "\t\t\treturns[state].append(G)\n",
        "\t\t\tvalue_table[state] = np.mean(returns[state])\n",
        "\n",
        "\treturn value_table\n",
        "\n",
        "# Define the number of episodes for MC evaluation\n",
        "num_episodes = 1000\n",
        "\n",
        "# Create a simple environment instance\n",
        "env = SimpleEnvironment(num_states=5)\n",
        "\n",
        "# Evaluate the policy\n",
        "v = monte_carlo_policy_evaluation(random_policy, env, num_episodes)\n",
        "\n",
        "print(\"The value table is:\")\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world environment\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = (3, 3)\n",
        "        self.num_actions = 4  # Up, Down, Left, Right\n",
        "        self.start_state = (0, 0)\n",
        "        self.goal_state = (2, 2)\n",
        "        self.actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]  # Right, Left, Up, Down\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state = (state[0] + action[0], state[1] + action[1])\n",
        "        if next_state[0] < 0 or next_state[0] >= self.grid_size[0] or \\\n",
        "                next_state[1] < 0 or next_state[1] >= self.grid_size[1]:\n",
        "            # Out of bounds, stay in the same state\n",
        "            return state, -1  # Reward of -1 for hitting the wall\n",
        "        return next_state, 0 if next_state == self.goal_state else -1  # 0 reward at goal, -1 otherwise\n",
        "\n",
        "# Define a random policy for the grid world\n",
        "def random_policy(state):\n",
        "    return np.random.choice(4)  # Randomly choose an action (0 to 3)\n",
        "\n",
        "# Monte Carlo simulation\n",
        "def monte_carlo_simulation(env, policy, num_episodes):\n",
        "    # Initialize Q-values and state visit counts\n",
        "    Q = np.zeros((env.grid_size[0], env.grid_size[1], env.num_actions))\n",
        "    N = np.zeros((env.grid_size[0], env.grid_size[1], env.num_actions))\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.start_state\n",
        "        episode = []\n",
        "        while state != env.goal_state:\n",
        "            action = policy(state)\n",
        "            next_state, reward = env.step(state, env.actions[action])\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0  # Return\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = reward + G\n",
        "            N[state[0], state[1], action] += 1\n",
        "            Q[state[0], state[1], action] += (1 / N[state[0], state[1], action]) * (G - Q[state[0], state[1], action])\n",
        "\n",
        "    return Q\n",
        "\n",
        "# Example usage\n",
        "env = GridWorld()\n",
        "Q_values = monte_carlo_simulation(env, random_policy, num_episodes=1000)\n",
        "print(\"Q-values:\")\n",
        "print(Q_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qygEN8TLpaZy",
        "outputId": "79245aae-f4a2-46ae-8aaf-93d19d1a1287"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values:\n",
            "[[[-25.358      -28.46938776 -26.93565332 -25.92005242]\n",
            "  [-21.80674847 -26.82553606 -23.70049261 -22.7681592 ]\n",
            "  [-23.17527174 -24.6846473  -21.12378303 -15.65703022]]\n",
            "\n",
            " [[-21.43409316 -25.56862745 -29.087      -22.51946721]\n",
            "  [-16.33247423 -25.36096257 -25.28244275 -16.42076503]\n",
            "  [-15.76908752 -21.10037879 -22.47826087   0.        ]]\n",
            "\n",
            " [[-15.44385027 -20.9527027  -25.56118143 -22.11509716]\n",
            "  [  0.         -21.97722567 -21.856      -16.68306011]\n",
            "  [  0.           0.           0.           0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XfoCwLEpoZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}